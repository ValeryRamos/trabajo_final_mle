{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***House Prices: Advanced Regression TechniquesÂ¶***\n\n![House Image](https://media.istockphoto.com/id/1450257988/vector/housing-price-rising-up.jpg?s=612x612&w=0&k=20&c=ZX2RDOCJL3FHMjgirJXE68oOfA6jp5iZNhAP8_QA3ik=)","metadata":{}},{"cell_type":"markdown","source":"# ***Table of Content***\n***1. About the Dataset***\n\n***2. Import Libraries***\n\n***3. Data Exploration & Visualization***\n\n***4. Data Preprocessing***\n\n***5. Model Building***\n\n***6. Conclussions***","metadata":{}},{"cell_type":"markdown","source":"# ***1. About the Dataset***\n\n\n*`Title`*: ***House Price Prediction - Advanced Regression Techniques***\n\n*`Context`*: *This dataset challenges participants to predict house prices in Ames, Iowa using 79 diverse explanatory variables. It demonstrates that factors beyond typical considerations like bedroom count significantly influence home prices. The competition aims to uncover the complex interplay of features affecting residential property values.*\n\n*`Project Goal`*\n- *Predict house sale prices based on given features*\n- *Each dataset row represents a house with its characteristics*\n- *Evaluation metric: Root-Mean-Squared-Error (RMSE) of log(predicted price) vs log(actual price)*\n- *Log-scale RMSE ensures equal weight for errors in expensive and inexpensive houses*","metadata":{}},{"cell_type":"markdown","source":"# ***2. Import Libraries***","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\n\n# System and utility libraries\nimport os\nimport warnings\nfrom datetime import datetime\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical and machine learning libraries\n## Core libraries\nfrom scipy import stats\nfrom scipy.special import boxcox1p\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score\n\n## Preprocessing\nfrom sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler\n\n## Machine learning models\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n## Dimensionality reduction and clustering\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# List input files\nprint(\"Input files:\\n\")\nfor file in os.listdir(\"./input\"):\n    print(file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***3. Data Exploration***","metadata":{}},{"cell_type":"markdown","source":"> ***Load the Dataset***","metadata":{}},{"cell_type":"code","source":"# submission data\ndf_submission = pd.read_csv(\"./input/sample_submission.csv\")\n\n# train data\ndf_train = pd.read_csv('./input/train.csv')\n\n# test data\ndf_test = pd.read_csv('./input/test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Checking the number of rows and columns***\n","metadata":{}},{"cell_type":"code","source":"# checking the number of rows and columns\nnum_train_rows, num_train_columns = df_train.shape\nnum_test_rows, num_test_columns = df_test.shape\n\nprint(\"Training Data:\")\nprint(f\"Number of Rows: {num_train_rows}\")\nprint(f\"Number of Columns: {num_train_columns}\\n\")\n\nprint(\"Test Data:\")\nprint(f\"Number of Rows: {num_test_rows}\")\nprint(f\"Number of Columns: {num_test_columns}\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Variable Categorization***\n\n- *Our dataset contains 81 attributes (including Id and SalePrice)*\n- *36 quantitative variables*\n- *43 categorical variables*","metadata":{}},{"cell_type":"code","source":"quantitative = [f for f in df_train.columns if df_train.dtypes[f] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nqualitative = [f for f in df_train.columns if df_train.dtypes[f] == 'object']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Checking for null values***","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 6))  # Set figure size to 12x6 inches\nmissing = (df_train.isnull().sum() / len(df_train) * 100).sort_values()\nmissing = missing[missing > 0]\nax = missing.plot.bar()\nax.axhline(y=50, color='r', linestyle='--', label='50% threshold')\nax.legend()\nplt.tight_layout()  # Adjust layout to prevent clipping\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Missing Values Analysis:***\n- *19 attributes have missing values*\n- *5 attributes have over 50% missing data*\n- *In most cases, NA likely indicates absence of the feature: No pool, No fence, No garage, No basement*\n\n*This suggests that missing values often represent meaningful information*\n*about the property, rather than just incomplete data.*","metadata":{}},{"cell_type":"markdown","source":"> ***Checking for duplicate values***","metadata":{}},{"cell_type":"code","source":"# Check for duplicate values\nduplicate_count = df_train.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count}\")\n\nif duplicate_count > 0:\n    print(\"\\nDuplicate rows:\")\n    print(df_train[df_train.duplicated(keep=False)])\nelse:\n    print(\"No duplicate rows found in the dataset.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***SalesPrice Distribution Analysis***","metadata":{}},{"cell_type":"code","source":"# Plot SalesPrice distribution\nplt.style.use('seaborn-whitegrid')\nsns.set_palette(\"deep\")\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.histplot(data=df_train, x='SalePrice', kde=True, color='navy', ax=ax)\nax.set_xlabel('Sale Price ($)', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.set_title('Distribution of Sale Prices', fontsize=14, fontweight='bold')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*SalePrice is not normally distributed. It needs transformation before regression. Log transformation works well, but Johnson distribution provides the best fit.*","metadata":{}},{"cell_type":"markdown","source":"> ***Visualize some of the features in dataset***","metadata":{}},{"cell_type":"code","source":"# Visualising Quantitative features in the dataset\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\n\nfor i, feature in enumerate(list(quantitative), 1):\n    plt.subplot(len(list(quantitative)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Spectral', data=df_train)\n    \n    plt.xlabel(f\"{feature}\", size=15, labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n        \n    plt.legend(loc='best', prop={'size' : 10})\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Normality Test for Quantitative Variables***","metadata":{}},{"cell_type":"code","source":"test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\nnormal = pd.DataFrame(df_train[quantitative])\nnormal = normal.apply(test_normality)\nprint(\"All normal\" if not normal.any() else \"Some not normal\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Our analysis shows that all quantitative variables are non-normally distributed. We should transform these variables before further analysis or modeling to improve statistical reliability and model performance.*","metadata":{}},{"cell_type":"markdown","source":"> ***Ordinal Encoding of Qualitative Features Based on SalePrice Mean***","metadata":{}},{"cell_type":"code","source":"def encode(frame, feature):\n    ordering = pd.DataFrame()\n    ordering['val'] = frame[feature].unique()\n    ordering.index = ordering.val\n    ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n    ordering = ordering.sort_values('spmean')\n    ordering['ordering'] = range(1, ordering.shape[0]+1)\n    ordering = ordering['ordering'].to_dict()\n    \n    for cat, o in ordering.items():\n        frame.loc[frame[feature] == cat, feature+'_E'] = o\n    \nqual_encoded = []\nfor q in qualitative:  \n    encode(df_train, q)\n    qual_encoded.append(q+'_E')\nprint(qual_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Correlation Analysis: Numerical, Categorical, and Cross-Feature Relationships***","metadata":{}},{"cell_type":"code","source":"# quantitative\nplt.figure(figsize=(12, 10))\ncorr = df_train[quantitative+['SalePrice']].corr()\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# qualitative\nplt.figure(figsize=(12, 10))\ncorr = df_train[qual_encoded+['SalePrice']].corr()\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cross feature relationship\nplt.figure(figsize=(12, 10))\ncorr = pd.DataFrame(np.zeros([len(quantitative)+1, len(qual_encoded)+1]), index=quantitative+['SalePrice'], columns=qual_encoded+['SalePrice'])\nfor q1 in quantitative+['SalePrice']:\n    for q2 in qual_encoded+['SalePrice']:\n        corr.loc[q1, q2] = df_train[q1].corr(df_train[q2])\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Advanced Data Visualization and Clustering analysis***","metadata":{}},{"cell_type":"code","source":"features = quantitative + qual_encoded\nX = df_train[features].fillna(0.).values\nmodel = TSNE(n_components=2, random_state=0, perplexity=50)\ntsne = model.fit_transform(X)\n\nstd = StandardScaler()\ns = std.fit_transform(X)\npca = PCA(n_components=30)\npca.fit(s)\npc = pca.transform(s)\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(pc)\n\nfr = pd.DataFrame({'tsne1': tsne[:,0], 'tsne2': tsne[:, 1], 'cluster': kmeans.labels_})\nsns.lmplot(data=fr, x='tsne1', y='tsne2', hue='cluster', fit_reg=False)\nprint(np.sum(pca.explained_variance_ratio_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***4. Data Preprocessing***","metadata":{}},{"cell_type":"markdown","source":"> ***Removing id column from train and test dataset***","metadata":{}},{"cell_type":"code","source":"df_train.drop(['Id'], axis=1, inplace=True)\ndf_test.drop(['Id'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Removing Outlier from GrLivArea***","metadata":{}},{"cell_type":"code","source":"df_train = df_train[df_train.GrLivArea < 4500]\ndf_train.reset_index(drop=True, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Log-transform the SalePrice***","metadata":{}},{"cell_type":"code","source":"df_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n# Separate target variable\ny = df_train['SalePrice'].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot SalesPrice distribution\nplt.style.use('seaborn-whitegrid')\nsns.set_palette(\"deep\")\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.histplot(data=df_train, x='SalePrice', kde=True, color='navy', ax=ax)\nax.set_xlabel('Sale Price ($)', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.set_title('Distribution of Sale Prices', fontsize=14, fontweight='bold')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***The log transformation has successfully normalized the SalePrice distribution, enhancing our model's performance and reliability***.","metadata":{}},{"cell_type":"markdown","source":"> ***Combined Feature Set for Preprocessing***","metadata":{}},{"cell_type":"code","source":"train_features = df_train.drop(['SalePrice'], axis=1)\ntest_features = df_test\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Type Conversion***","metadata":{}},{"cell_type":"code","source":"features['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Missing Value Imputation***","metadata":{}},{"cell_type":"code","source":"# Fill specific categorical features\ncategorical_fills = {\n    'Functional': 'Typ',\n    'Electrical': 'SBrkr',\n    'KitchenQual': 'TA',\n    'PoolQC': 'None',\n    'Exterior1st': features['Exterior1st'].mode()[0],\n    'Exterior2nd': features['Exterior2nd'].mode()[0],\n    'SaleType': features['SaleType'].mode()[0]\n}\nfeatures = features.fillna(categorical_fills)\n\n# Fill numeric garage features with 0\nfeatures[['GarageYrBlt', 'GarageArea', 'GarageCars']] = features[['GarageYrBlt', 'GarageArea', 'GarageCars']].fillna(0)\n\n# Fill categorical garage and basement features with 'None'\ngarage_basement_cols = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nfeatures[garage_basement_cols] = features[garage_basement_cols].fillna('None')\n\n# Fill MSZoning based on MSSubClass mode\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# Fill remaining categorical columns with 'None' and numeric columns with 0\nfeatures = features.apply(lambda x: x.fillna('None') if x.dtype == 'object' else x.fillna(0))\n\n# Special handling for LotFrontage\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Normalizing Skewed Numerical Features Using Power Transformation***","metadata":{}},{"cell_type":"code","source":"# Calculate skewness and identify highly skewed features\nnumeric_features = features.select_dtypes(include=['number']).columns\nskew_features = features[numeric_features].apply(lambda x: stats.skew(x))\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\n# Normalize skewed features using PowerTransformer\npt = PowerTransformer(method='yeo-johnson', standardize=False)\nfeatures[skew_index] = pt.fit_transform(features[skew_index])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Creating New Features***","metadata":{}},{"cell_type":"code","source":"# Let's supercharge our dataset with some awesome feature combos!\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's spice things up with some yes/no features!\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Feature Encoding***","metadata":{}},{"cell_type":"code","source":"# One-Hot Encoding\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nfinal_features.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the final_features into training (X) and submission (X_sub) datasets\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(y):, :]\nX.shape, y.shape, X_sub.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Removing Outliers and Low Variance Features***","metadata":{}},{"cell_type":"code","source":"# Remove outliers\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\n# Remove features with >99.94% zeros\noverfit = [i for i in X.columns if X[i].value_counts().iloc[0] / len(X) * 100 > 99.94]\nX = X.drop(overfit, axis=1)\nX_sub = X_sub.drop(overfit, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape, X_sub.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***5. Model Building***","metadata":{}},{"cell_type":"markdown","source":"> ***Cross Validation Setup***","metadata":{}},{"cell_type":"code","source":"# KFold Cross-Validation\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# RMSE Function\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n# Cross-Validation RMSE Function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter lists for different regression models\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Model Initialization***","metadata":{}},{"cell_type":"code","source":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=int(1e7), alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=int(1e7), alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Stacking Model***","metadata":{}},{"cell_type":"code","source":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Model Evaluation: Cross-Validation RMSE Scores***","metadata":{}},{"cell_type":"code","source":"# Evaluate and compare multiple regression models using cross-validation RMSE scores\nscore = cv_rmse(ridge)\nprint(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), \"\\n\")\n\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), \"\\n\")\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), \"\\n\")\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), \"\\n\")\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), \"\\n\")\n\nscore = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), \"\\n\")\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), \"\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Model Training and Fitting***","metadata":{}},{"cell_type":"code","source":"# Model Fitting\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Blending Model***","metadata":{}},{"cell_type":"code","source":"# Weighted ensemble of multiple regression models\ndef blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('RMSE score on train data:')\nprint(rmse(y, blend_models_predict(X)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Create submission file***","metadata":{}},{"cell_type":"code","source":"# Predict using the blended model\nblend_predictions = blend_models_predict(X_sub)\nblend_predictions = np.expm1(blend_predictions)\n\n# Create a submission DataFrame\nsubmission = pd.DataFrame({\n    'Id': df_submission['Id'],\n    'SalePrice': blend_predictions\n})\n\n# Save the submission file\nsubmission.to_csv(\"submission/blended_model_submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***6. Conclusion***\n\n*Our blended model approach, combining multiple regression techniques including Elastic Net, Lasso, Ridge, SVR, Gradient Boosting, XGBoost, LightGBM, and stacked models, has demonstrated robust performance in predicting house prices. The weighted ensemble leverages the strengths of each individual model, potentially mitigating their individual weaknesses.*\n\n*The RMSE score on the training data indicates a good fit, suggesting that our model has captured significant patterns in the housing market data. However, it's important to note that the true test of our model's efficacy will be its performance on unseen data in the Kaggle competition.*\n\n*This approach showcases the power of ensemble methods in tackling complex regression problems, particularly in the real estate domain where numerous factors influence property values. Future improvements could involve fine-tuning the model weights, exploring additional feature engineering, or incorporating more advanced techniques like neural networks.*\n","metadata":{}}]}